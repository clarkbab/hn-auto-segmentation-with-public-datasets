{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HN Segmentation with Transfer Learning\n",
    "\n",
    "From journal article \"Transfer learning for auto-segmentation of 17 organs-at-risk in the head and neck: bridging the gap between institutional and public datasets\", published in ...\n",
    "\n",
    "To complete this experiment, you must have access to a large institutional CT head and neck auto-segmentation dataset or similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: These first few commands should be run in a terminal, not within the notebook\n",
    "as we've yet to set up a suitable Jupyter notebook kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python v3.10.4 or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and activate 'transfer-learning' virtual environment.\n",
    "$ python -m venv ~/venvs/transfer-learning\n",
    "$ source ~/venvs/transfer-learning/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Jupyter kernel pointing to virtual environment.\n",
    "$ pip install ipykernel\n",
    "$ python -m ipykernel install --user --name=transfer-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Commands from here on are run in this Jupyter notebook. You should\n",
    "restart the Jupyter notebook to ensure the kernel loads, and then select\n",
    "the 'transfer-learning' kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/home/baclark/venvs/transfer-learning/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required Python packages.\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data folder somewhere, and edit/export the following environment\n",
    "# variables:\n",
    "#   HNAS_CODE: The filepath pointing to \"hn-segmentation-with-transfer-learning\" code.\n",
    "#   HNAS_DATA: The filepath to your new data folder.\n",
    "! export HNAS_CODE=\"/absolute/path/to/hn-segmentation-with-transfer-learning\"\n",
    "! export HNAS_DATA=\"/absolute/path/to/data/folder\"\n",
    "! export HNAS_CODE=\"/data/projects/punim1413/hn-segmentation-with-transfer-learning/\"\n",
    "! export HNAS_DATA=\"/data/projects/punim1413/transfer-learning/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Public Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to avoid training the public models yourself, you can find them on [Zenodo](/link/after/publication). You can then skip the \"Preparing Public Data\" and \"Training Public Models\" sections.\n",
    "\n",
    "Download the public datasets from the Cancer Imaging Archive.\n",
    "- HN1: Available from [Head-Neck-Radiomics-HN1](https://wiki.cancerimagingarchive.net/display/Public/Head-Neck-Radiomics-HN1).\n",
    "- HNPCT: Available from [Head-Neck-PET-CT](https://wiki.cancerimagingarchive.net/display/Public/Head-Neck-PET-CT).\n",
    "- HNSCC: Available from [HNSCC](https://wiki.cancerimagingarchive.net/display/Public/HNSCC).\n",
    "- OPC: Available from [OPC-Radiomics](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=33948764).\n",
    "\n",
    "For each of these datasets, drop the downloaded data into the corresponding \"data\" folder\n",
    "created with the following structure. Folder structure within \"data\" doesn't matter as dataset\n",
    "indexing will occur when you first query the `DICOMDataset`.\n",
    "\n",
    "```\n",
    "<HNAS_DATA>\n",
    "    /datasets\n",
    "        /dicom\n",
    "            /HN1\n",
    "                /data\n",
    "            /HNPCT\n",
    "                /data\n",
    "            /HNSCC\n",
    "                /data\n",
    "            /OPC\n",
    "                /data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/data/projects/punim1413/transfer-learning/datasets/dicom/HN1/region-map.csv': File exists\n",
      "ln: failed to create symbolic link '/data/projects/punim1413/transfer-learning/datasets/dicom/HNPCT/region-map.csv': File exists\n",
      "ln: failed to create symbolic link '/data/projects/punim1413/transfer-learning/datasets/dicom/HNSCC/region-map.csv': File exists\n",
      "ln: failed to create symbolic link '/data/projects/punim1413/transfer-learning/datasets/dicom/OPC/region-map.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "# We will need to provide a mapping between the organ-at-risk names in the public datasets\n",
    "# and the conventions that we will use in this package. This is carried out by creation a \n",
    "# 'region-map.csv' file in the root folder of the `DICOMDataset`. These files have already\n",
    "# been created and just need to by symlinked to the correct locations.\n",
    "\n",
    "! ln -s $HNAS_CODE/hnas/dataset/dicom/files/region-maps/hn1-region-map.csv $HNAS_DATA/datasets/dicom/HN1/region-map.csv\n",
    "! ln -s $HNAS_CODE/hnas/dataset/dicom/files/region-maps/hnpct-region-map.csv $HNAS_DATA/datasets/dicom/HNPCT/region-map.csv\n",
    "! ln -s $HNAS_CODE/hnas/dataset/dicom/files/region-maps/hnscc-region-map.csv $HNAS_DATA/datasets/dicom/HNSCC/region-map.csv\n",
    "! ln -s $HNAS_CODE/hnas/dataset/dicom/files/region-maps/opc-region-map.csv $HNAS_DATA/datasets/dicom/OPC/region-map.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/data/projects/punim1413/transfer-learning/datasets/dicom/HNPCT/region-dups.csv': File exists\n",
      "ln: failed to create symbolic link '/data/projects/punim1413/transfer-learning/datasets/dicom/HNSCC/region-dups.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This mapping process fails if there are multiple labels that would map to the same\n",
    "# name. When this would occur, we need to register the duplicate labels.\n",
    "\n",
    "! ln -s $HNAS_CODE/hnas/dataset/dicom/files/region-maps/hnpct-region-dups.csv $HNAS_DATA/datasets/dicom/HNPCT/region-dups.csv\n",
    "! ln -s $HNAS_CODE/hnas/dataset/dicom/files/region-maps/hnscc-region-dups.csv $HNAS_DATA/datasets/dicom/HNSCC/region-dups.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --export=ALL,DATASET=HN1 scripts/slurm/steps/4/template.slurm\n",
      "sbatch --export=ALL,DATASET=HNPCT scripts/slurm/steps/4/template.slurm\n",
      "sbatch --export=ALL,DATASET=HNSCC scripts/slurm/steps/4/template.slurm\n",
      "sbatch --export=ALL,DATASET=OPC scripts/slurm/steps/4/template.slurm\n"
     ]
    }
   ],
   "source": [
    "# Process DICOM data to NIFTI.\n",
    "# NIFTI stores medical imaging data in a compact format and removes extraneous details \n",
    "# that are present in DICOM files.\n",
    "\n",
    "# Option A (preferred).\n",
    "# Creates jobs to process the data on a slurm cluster.\n",
    "! python scripts/slurm/steps/4/create_jobs.py       # Creates 4 slurm jobs.\n",
    "\n",
    "# Option B.\n",
    "# Runs processing on local machine.\n",
    "#! python scripts/python/steps/4.py       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process NIFTI data to training data.\n",
    "\n",
    "# Option A (preferred).\n",
    "# Creates jobs to process the data on a slurm cluster.\n",
    "! python scripts/slurm/steps/5/create_jobs.py       # Creates 8 slurm jobs.\n",
    "\n",
    "# Option B.\n",
    "# Runs processing on local machine.\n",
    "#! python scripts/python/steps/5.py       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Public Models\n",
    "\n",
    "If you would like to avoid training the public models yourself, you can find them on [Zenodo](/link/after/publication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a public localiser and segmenter network per organ-at-risk.\n",
    "# NOTE: Some editing of the slurm template files will be necessary to connect to your\n",
    "# GPU partition. Training is configured to use [wandb](https://wandb.ai/) reporting by\n",
    "# default, but this must be enabled by setting USE_LOGGER=True in the templates.\n",
    "\n",
    "# Option A (preferred).\n",
    "# Creates jobs to train the networks on a slurm cluster.\n",
    "! python scripts/slurm/steps/6/create_jobs.py       # Creates 34 slurm jobs.\n",
    "\n",
    "# Option B.\n",
    "# Runs training on local machine.\n",
    "#! python scripts/python/steps/6.py       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training can be resumed upon failure with the following scripts.\n",
    "\n",
    "# Option A (preferred).\n",
    "# Creates jobs to train the networks on a slurm cluster.\n",
    "! python scripts/slurm/steps/6/create_resume_jobs.py       # Creates 34 slurm jobs.\n",
    "\n",
    "# Option B.\n",
    "# Runs training on local machine.\n",
    "#! python scripts/python/steps/6_resume.py       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Institutional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create institutional dataset (e.g \"INST\") in a similar manner to the public datasets,\n",
    "by dropping all data into the \"data\" folder.\n",
    "\n",
    "```\n",
    "<HNAS_DATA>\n",
    "    /datasets\n",
    "        /dicom\n",
    "            /INST\n",
    "                /data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process DICOM data to NIFTI.\n",
    "# NIFTI stores medical imaging data in a compact format and removes extraneous details \n",
    "# that are present in DICOM files.\n",
    "\n",
    "# Option A (preferred).\n",
    "# Creates jobs to process the data on a slurm cluster.\n",
    "! python scripts/slurm/steps/8/create_jobs.py       # Creates 1 slurm jobs.\n",
    "\n",
    "# Option B.\n",
    "# Runs processing on local machine.\n",
    "#! python scripts/python/steps/8.py       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process NIFTI data to training data.\n",
    "\n",
    "# Option A (preferred).\n",
    "# Creates jobs to process the data on a slurm cluster.\n",
    "! python scripts/slurm/steps/9/create_jobs.py       # Creates 1 slurm jobs.\n",
    "\n",
    "# Option B.\n",
    "# Runs processing on local machine.\n",
    "#! python scripts/python/steps/9.py       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Institutional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This script will create 595 (17 organs-at-risk x 7 dataset sizes x 5-fold cross-validation)\n",
    "# slurm jobs!!! You will need to modify the \"regions\", \"n_trains\", and \"test_folds\" of the script to\n",
    "# initiate the jobs in smaller batches to avoid queue limits.\n",
    "\n",
    "# Train an institutional localiser and segmenter per organ-at-risk.\n",
    "# NOTE: Some editing of the slurm template files will be necessary to connect to your\n",
    "# GPU partition. Training is configured to use [wandb](https://wandb.ai/) reporting by\n",
    "# default, but this must be enabled by setting USE_LOGGER=True in the templates.\n",
    "\n",
    "# Option A (preferred).\n",
    "# Creates jobs to train the networks on a slurm cluster.\n",
    "! python scripts/slurm/steps/10/create_jobs.py       # Creates 595 slurm jobs.\n",
    "\n",
    "# Option B.\n",
    "# Runs training on local machine.\n",
    "#! python scripts/python/steps/10.py       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfer-learning",
   "language": "python",
   "name": "transfer-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
